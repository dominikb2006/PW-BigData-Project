{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "483ca9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYSPARK_DRIVER_PYTHON=jupyter\n",
      "env: PYSPARK_DRIVER_PYTHON_OPTS=notebook\n",
      "env: PYSPARK_PYTHON=python\n",
      "env: OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES\n"
     ]
    }
   ],
   "source": [
    "%env PYSPARK_DRIVER_PYTHON=jupyter\n",
    "%env PYSPARK_DRIVER_PYTHON_OPTS=notebook\n",
    "%env PYSPARK_PYTHON=python\n",
    "%env OBJC_DISABLE_INITIALIZE_FORK_SAFETY = YES\n",
    "\n",
    "# HADOOP_HOME = C:\\Hadoop\n",
    "# JAVA_HOME = C:\\Java\\jdk-11.0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "42f53f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, IDF, IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import to_timestamp, from_unixtime, when, col, regexp_replace, lower, udf, concat, lit\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import IntegerType, DoubleType, ArrayType, StringType\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "class Stemmer(Transformer, HasInputCol, HasOutputCol):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(Stemmer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def setInputCol(self, value):\n",
    "        \"\"\"\n",
    "        Sets the value of :py:attr:`inputCol`.\n",
    "        \"\"\"\n",
    "        return self._set(inputCol=value)\n",
    "\n",
    "    def setOutputCol(self, value):\n",
    "        \"\"\"\n",
    "        Sets the value of :py:attr:`outputCol`.\n",
    "        \"\"\"\n",
    "        return self._set(outputCol=value)\n",
    "        \n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        \n",
    "        def stem_words(words):\n",
    "            return [PorterStemmer().stem(w) for w in words]\n",
    "        \n",
    "        t = ArrayType(StringType())\n",
    "        out_col = self.getOutputCol()\n",
    "        in_col = df[self.getInputCol()]\n",
    "        return df.withColumn(out_col, udf(f=stem_words, returnType=t)(in_col))\n",
    "    \n",
    "    \n",
    "def preprocessing(spark_session, path: str, is_test: bool):\n",
    "    # read file\n",
    "    df = spark_session.read.json(path=path)\n",
    "    \n",
    "    #change columns\n",
    "    df = df.withColumn(\"label\", df[\"overall\"].cast(IntegerType()))\n",
    "    # df = df.withColumn(\"asin\", df[\"asin\"].cast(DoubleType()))\n",
    "    # df = df.withColumn(\"vote\", when(col(\"vote\") == \"none\", None).otherwise(col(\"vote\")).cast(\"double\"))\n",
    "    # df = df.withColumn('style', df[\"style.Format:\"]) \n",
    "    # df = df.withColumn('unixReviewTime', from_unixtime(df['unixReviewTime']))\n",
    "    # df = df.withColumn(\"reviewTime\", to_timestamp(df.reviewTime, 'MM d, yyyy'))\n",
    "\n",
    "    df = df.drop('asin', 'image', 'reviewTime', 'reviewerID', 'reviewerName', 'reviewTime', 'unixReviewTime', 'overall'\\\n",
    "                , 'style', 'summary', 'verified', 'vote') \n",
    "    \n",
    "    df = df.withColumn('reviewText', regexp_replace('reviewText', \"\\S+@\\S+\\s\", ' '))\\\n",
    "                .withColumn('reviewText', regexp_replace('reviewText', \"\\n|\\t\", ' '))\\\n",
    "                .withColumn('reviewText', regexp_replace('reviewText', \"\\S*\\d+\\S*\", ' '))\\\n",
    "                .withColumn('reviewText', regexp_replace('reviewText', \"\\s\\W*\\w\\W*\\s\", ' '))\\\n",
    "                .withColumn('reviewText', regexp_replace('reviewText', \"\\W+\", ' '))\n",
    "    \n",
    "    df = df.withColumn('reviewText', lower('reviewText'))\n",
    "    # fill null values\n",
    "    df = df.na.fill(\"null\",[\"reviewText\"]) \n",
    "    \n",
    "    if is_test:\n",
    "        df = df.limit(100)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e75b8c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "WITH_INDEXERS = True\n",
    "IS_TEST = True\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"ML\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"6g\")\\\n",
    "    .config('spark.executor.cores','1')\\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "path = r'C:\\Users\\dominik.brys\\OneDrive - Accenture\\DB\\PW-Big_Data\\Projekt\\Books_5\\Books_5_limited_v2.json'\n",
    "\n",
    "df = preprocessing(spark, path, is_test=IS_TEST)\n",
    "\n",
    "# df.show()\n",
    "\n",
    "preprocessing_pipeline = Pipeline(\n",
    "    stages=[\n",
    "        RegexTokenizer(inputCol=\"reviewText\", outputCol=\"words\", pattern=\"\\\\W\"),\n",
    "        StopWordsRemover(inputCol='words', outputCol='words_cleaned'),\n",
    "        Stemmer(inputCol='words_cleaned', outputCol='words_stemmed'),\n",
    "        CountVectorizer(inputCol=\"words_stemmed\", outputCol=\"term_freq\", minDF=10.0), #, maxDF=0.5),\n",
    "        IDF(inputCol=\"term_freq\", outputCol=\"tfidf\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessing_model = preprocessing_pipeline.fit(df)\n",
    "df = preprocessing_model.transform(df)\n",
    "\n",
    "# df.show(2)\n",
    "\n",
    "df = df.select(['tfidf', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8ead6dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.217391\n",
      "{Param(parent='RandomForestClassifier_8d68091fd606', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 10, Param(parent='RandomForestClassifier_8d68091fd606', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 5, Param(parent='RandomForestClassifier_8d68091fd606', name='numTrees', doc='Number of trees to train (>= 1).'): 50, Param(parent='RandomForestClassifier_8d68091fd606', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'gini'}\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = df.randomSplit([0.75, 0.25], seed=123)\n",
    "\n",
    "\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(r_forest.maxDepth, [10, 20, 30]) \\\n",
    "    .addGrid(r_forest.maxBins, [5, 10, 25])\\\n",
    "    .addGrid(r_forest.numTrees, [50, 100, 200])\\\n",
    "    .addGrid(r_forest.impurity, ['gini','entropy'])\\\n",
    "    .build()\n",
    "\n",
    "if WITH_INDEXERS is True:\n",
    "    labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(df)\n",
    "    featureIndexer = VectorIndexer(inputCol=\"tfidf\", outputCol=\"indexedFeatures\", maxCategories=4).fit(df)\n",
    "\n",
    "    r_forest = RandomForestClassifier(bootstrap=True, labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "    cv = CrossValidator(estimator=r_forest,\n",
    "                        estimatorParamMaps=param_grid,\n",
    "                        evaluator=evaluator,\n",
    "                        numFolds=5)\n",
    "\n",
    "    labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=labelIndexer.labels)\n",
    "\n",
    "\n",
    "    pipeline = Pipeline(stages=[labelIndexer, \n",
    "                                featureIndexer, \n",
    "                                cv,\n",
    "                                labelConverter\n",
    "                               ])\n",
    "    cv_stage = 2\n",
    "    \n",
    "else:\n",
    "    r_forest = RandomForestClassifier(bootstrap=True, \n",
    "                                      labelCol=\"label\", featuresCol=\"tfidf\")\n",
    "\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\", predictionCol=\"prediction\", \n",
    "        metricName=\"accuracy\")\n",
    "\n",
    "    cv = CrossValidator(estimator=r_forest,\n",
    "                        estimatorParamMaps=param_grid,\n",
    "                        evaluator=evaluator,\n",
    "                        numFolds=5)\n",
    "\n",
    "\n",
    "    pipeline = Pipeline(stages=[cv])\n",
    "    cv_stage = 0\n",
    "\n",
    "\n",
    "model = pipeline.fit(train_set)\n",
    "predictions = model.transform(test_set)\n",
    "\n",
    "\n",
    "# Select example rows to display.\n",
    "# predictions.select(\"prediction\", \"label\", \"tfidf\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "# predictions.show()\n",
    "\n",
    "hyperparams = model.stages[cv_stage].getEstimatorParamMaps()[np.argmax(model.stages[cv_stage].avgMetrics)]\n",
    "print(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "966fc129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'maxDepth': 10}, {'maxBins': 5}, {'numTrees': 50}, {'impurity': 'gini'}]\n"
     ]
    }
   ],
   "source": [
    "hyperparams = model.stages[cv_stage].getEstimatorParamMaps()[np.argmax(model.stages[cv_stage].avgMetrics)]\n",
    "\n",
    "import re\n",
    "\n",
    "hyper_list = []\n",
    "\n",
    "for i in range(len(hyperparams.items())):\n",
    "    hyper_name = re.search(\"name='(.+?)'\", str([x for x in hyperparams.items()][i])).group(1)\n",
    "    hyper_value = [x for x in hyperparams.items()][i][1]\n",
    "\n",
    "    hyper_list.append({hyper_name: hyper_value})\n",
    "\n",
    "print(hyper_list)\n",
    "\n",
    "# print(model.stages[0].bestModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2d6ac871",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(model.stages[0].getEstimatorParamMaps())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8da5689d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Param(parent='RandomForestClassifier_e1d7e852a88a', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 10, Param(parent='RandomForestClassifier_e1d7e852a88a', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 5, Param(parent='RandomForestClassifier_e1d7e852a88a', name='numTrees', doc='Number of trees to train (>= 1).'): 50}\n"
     ]
    }
   ],
   "source": [
    "print(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d102ac5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
