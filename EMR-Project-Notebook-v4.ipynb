{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a99ae81d-cf6a-4a8b-b383-f885c0325a14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYSPARK_DRIVER_PYTHON=jupyter\n",
      "env: PYSPARK_DRIVER_PYTHON_OPTS=notebook\n",
      "env: PYSPARK_PYTHON=python\n",
      "env: OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES\n"
     ]
    }
   ],
   "source": [
    "%env PYSPARK_DRIVER_PYTHON=jupyter\n",
    "%env PYSPARK_DRIVER_PYTHON_OPTS=notebook\n",
    "%env PYSPARK_PYTHON=python\n",
    "%env OBJC_DISABLE_INITIALIZE_FORK_SAFETY = YES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7619ed82-a761-40d3-8d58-9d2658c5657b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    852\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 853\u001b[1;33m                 \u001b[0mitem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    854\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DOMINI~1.BRY\\AppData\\Local\\Temp/ipykernel_21292/745579201.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\DOMINI~1.BRY\\AppData\\Local\\Temp/ipykernel_21292/745579201.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_spark_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m     \u001b[0mprocess_book_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mINPUT_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOUTPUT_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWITH_INDEXERS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIS_TEST\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIS_LOCAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\DOMINI~1.BRY\\AppData\\Local\\Temp/ipykernel_21292/745579201.py\u001b[0m in \u001b[0;36mprocess_book_data\u001b[1;34m(spark, input_path, output_path, with_indexers, is_test, is_local)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    159\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m             raise TypeError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    112\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# must be an Estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m                     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    159\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m             raise TypeError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    687\u001b[0m                 \u001b[0minheritable_thread_target\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m                 _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam))\n\u001b[1;32m--> 689\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubModel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    690\u001b[0m                 \u001b[0mmetrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmetric\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnFolds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    856\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 858\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    859\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m                     \u001b[0mitem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# pyspark_job.py\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, IDF, IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import to_timestamp, from_unixtime, when, col, regexp_replace, lower, udf, concat, lit\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import IntegerType, DoubleType, ArrayType, StringType\n",
    "\n",
    "\n",
    "class Stemmer(Transformer, HasInputCol, HasOutputCol):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(Stemmer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def setInputCol(self, value):\n",
    "        \"\"\"\n",
    "        Sets the value of :py:attr:`inputCol`.\n",
    "        \"\"\"\n",
    "        return self._set(inputCol=value)\n",
    "\n",
    "    def setOutputCol(self, value):\n",
    "        \"\"\"\n",
    "        Sets the value of :py:attr:`outputCol`.\n",
    "        \"\"\"\n",
    "        return self._set(outputCol=value)\n",
    "        \n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        \n",
    "        def stem_words(words):\n",
    "            return [PorterStemmer().stem(w) for w in words]\n",
    "        \n",
    "        t = ArrayType(StringType())\n",
    "        out_col = self.getOutputCol()\n",
    "        in_col = df[self.getInputCol()]\n",
    "        return df.withColumn(out_col, udf(f=stem_words, returnType=t)(in_col))\n",
    "    \n",
    "    \n",
    "def preprocessing(spark_session, path: str, is_test: bool):\n",
    "    # read file\n",
    "    df = spark_session.read.json(path=path)\n",
    "    \n",
    "    #change columns\n",
    "    df = df.withColumn(\"label\", df[\"overall\"].cast(IntegerType()))\n",
    "    # df = df.withColumn(\"asin\", df[\"asin\"].cast(DoubleType()))\n",
    "    # df = df.withColumn(\"vote\", when(col(\"vote\") == \"none\", None).otherwise(col(\"vote\")).cast(\"double\"))\n",
    "    # df = df.withColumn('style', df[\"style.Format:\"]) \n",
    "    # df = df.withColumn('unixReviewTime', from_unixtime(df['unixReviewTime']))\n",
    "    # df = df.withColumn(\"reviewTime\", to_timestamp(df.reviewTime, 'MM d, yyyy'))\n",
    "\n",
    "    df = df.drop('asin', 'image', 'reviewTime', 'reviewerID', 'reviewerName', 'reviewTime', 'unixReviewTime', 'overall'\\\n",
    "                , 'style', 'summary', 'verified', 'vote') \n",
    "    \n",
    "    df = df.withColumn('reviewText', regexp_replace('reviewText', \"\\S+@\\S+\\s\", ' '))\\\n",
    "                .withColumn('reviewText', regexp_replace('reviewText', \"\\n|\\t\", ' '))\\\n",
    "                .withColumn('reviewText', regexp_replace('reviewText', \"\\S*\\d+\\S*\", ' '))\\\n",
    "                .withColumn('reviewText', regexp_replace('reviewText', \"\\s\\W*\\w\\W*\\s\", ' '))\\\n",
    "                .withColumn('reviewText', regexp_replace('reviewText', \"\\W+\", ' '))\n",
    "    \n",
    "    df = df.withColumn('reviewText', lower('reviewText'))\n",
    "    # fill null values\n",
    "    df = df.na.fill(\"null\",[\"reviewText\"]) \n",
    "    \n",
    "    if is_test:\n",
    "        df = df.limit(100)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "    \n",
    "def create_spark_session():\n",
    "    \"\"\"Create spark session.\n",
    "Returns:\n",
    "        spark (SparkSession) - spark session connected to AWS EMR\n",
    "            cluster\n",
    "    \"\"\"\n",
    "    spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"ML\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "    # .config('spark.driver.host','127.0.0.1') \\\n",
    "    # .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    # .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    # .config('spark.executor.cores', '2') \\\n",
    "    \n",
    "    return spark\n",
    "\n",
    "\n",
    "def process_book_data(spark, input_path, output_path, with_indexers, is_test, is_local):\n",
    "\n",
    "    df = preprocessing(spark, input_path, is_test)\n",
    "\n",
    "\n",
    "    preprocessing_pipeline = Pipeline(\n",
    "        stages=[\n",
    "            RegexTokenizer(inputCol=\"reviewText\", outputCol=\"words\", pattern=\"\\\\W\"),\n",
    "            StopWordsRemover(inputCol='words', outputCol='words_cleaned'),\n",
    "            Stemmer(inputCol='words_cleaned', outputCol='words_stemmed'),\n",
    "            CountVectorizer(inputCol=\"words_stemmed\", outputCol=\"term_freq\", minDF=10.0), #, maxDF=0.5),\n",
    "            IDF(inputCol=\"term_freq\", outputCol=\"tfidf\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preprocessing_model = preprocessing_pipeline.fit(df)\n",
    "    df = preprocessing_model.transform(df)\n",
    "\n",
    "    # df.show(2)\n",
    "\n",
    "    df = df.select(['tfidf', 'label'])\n",
    "    train_set, test_set = df.randomSplit([0.75, 0.25], seed=123)\n",
    "\n",
    "\n",
    "\n",
    "    if with_indexers is True:\n",
    "        labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(df)\n",
    "        featureIndexer = VectorIndexer(inputCol=\"tfidf\", outputCol=\"indexedFeatures\", maxCategories=4).fit(df)\n",
    "\n",
    "        r_forest = RandomForestClassifier(bootstrap=True, labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n",
    "        evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "        param_grid = ParamGridBuilder() \\\n",
    "        .addGrid(r_forest.maxDepth, [10, 20]) \\\n",
    "        .addGrid(r_forest.maxBins, [5, 10]) \\\n",
    "        .addGrid(r_forest.numTrees, [50, 100, 200]) \\\n",
    "        .addGrid(r_forest.impurity, ['gini','entropy']) \\\n",
    "        .build()\n",
    "                \n",
    "        cv = CrossValidator(estimator=r_forest,\n",
    "                            estimatorParamMaps=param_grid,\n",
    "                            evaluator=evaluator,\n",
    "                            numFolds=5)\n",
    "\n",
    "        labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=labelIndexer.labels)\n",
    "\n",
    "        \n",
    "        pipeline = Pipeline(stages=[labelIndexer, \n",
    "                                    featureIndexer, \n",
    "                                    cv,\n",
    "                                    labelConverter\n",
    "                                   ])\n",
    "        cv_stage = 2\n",
    "\n",
    "    else:\n",
    "        r_forest = RandomForestClassifier(bootstrap=True, labelCol=\"label\", featuresCol=\"tfidf\")\n",
    "\n",
    "        evaluator = MulticlassClassificationEvaluator(\n",
    "            labelCol=\"label\", predictionCol=\"prediction\", \n",
    "            metricName=\"accuracy\")\n",
    "\n",
    "        \n",
    "        param_grid = ParamGridBuilder() \\\n",
    "        .addGrid(r_forest.maxDepth, [10, 20]) \\\n",
    "        .addGrid(r_forest.maxBins, [5, 10]) \\\n",
    "        .addGrid(r_forest.numTrees, [50, 100, 200]) \\\n",
    "        .addGrid(r_forest.impurity, ['gini','entropy']) \\\n",
    "        .build()\n",
    "        \n",
    "        cv = CrossValidator(estimator=r_forest,\n",
    "                            estimatorParamMaps=param_grid,\n",
    "                            evaluator=evaluator,\n",
    "                            numFolds=5)\n",
    "\n",
    "\n",
    "        pipeline = Pipeline(stages=[cv])\n",
    "        cv_stage = 0\n",
    "\n",
    "\n",
    "    model = pipeline.fit(train_set)\n",
    "    predictions = model.transform(test_set)\n",
    "    \n",
    "    hyperparams = model.stages[cv_stage].getEstimatorParamMaps()[np.argmax(model.stages[cv_stage].avgMetrics)]\n",
    "\n",
    "\n",
    "    hyper_list = []\n",
    "\n",
    "    for i in range(len(hyperparams.items())):\n",
    "        hyper_name = re.search(\"name='(.+?)'\", str([x for x in hyperparams.items()][i])).group(1)\n",
    "        hyper_value = [x for x in hyperparams.items()][i][1]\n",
    "\n",
    "        hyper_list.append({hyper_name: hyper_value})\n",
    "\n",
    "\n",
    "    params = spark.read.json(spark.sparkContext.parallelize(hyper_list))\n",
    "    \n",
    "    if not is_local:\n",
    "        params.write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\n",
    "            path = output_path, mode = \"overwrite\")\n",
    "    else:\n",
    "        params.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "def main():\n",
    "#     input_path = ('s3://amazon-reviews-pds/parquet/' +\n",
    "#                   'product_category=Books/*.parquet')\n",
    "#     output_path = 's3://spark-tutorial-bwl/book-aggregates'\n",
    "    \n",
    "    WITH_INDEXERS = False\n",
    "    IS_TEST = False\n",
    "    IS_LOCAL = True\n",
    "    \n",
    "    \n",
    "    \n",
    "    if IS_LOCAL:\n",
    "        INPUT_PATH = r'C:\\Users\\dominik.brys\\OneDrive - Accenture\\DB\\PW-Big_Data\\Projekt\\Books_5\\Books_5_limited_v2.json'\n",
    "        OUTPUT_PATH = r'C:\\Users\\dominik.brys\\OneDrive - Accenture\\DB\\PW-Big_Data\\Projekt\\output\\model'\n",
    "    else:\n",
    "        bucket = 's3://emr-project-v1-db'\n",
    "        input_file = '/Books_5_limited_v2.json'\n",
    "        output_file = '/output_params'\n",
    "        INPUT_PATH = bucket + input_file\n",
    "        OUTPUT_PATH = bucket + output_file\n",
    "    \n",
    "    \n",
    "    spark = create_spark_session()\n",
    "    process_book_data(spark, INPUT_PATH, OUTPUT_PATH, WITH_INDEXERS, IS_TEST, IS_LOCAL)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee5ea7ce-dcce-4280-a20f-c18fafa16d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIONAL\n",
    "# params.show()\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
