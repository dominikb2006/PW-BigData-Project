{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a99ae81d-cf6a-4a8b-b383-f885c0325a14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYSPARK_DRIVER_PYTHON=jupyter\n",
      "env: PYSPARK_DRIVER_PYTHON_OPTS=notebook\n",
      "env: PYSPARK_PYTHON=python\n",
      "env: OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES\n"
     ]
    }
   ],
   "source": [
    "%env PYSPARK_DRIVER_PYTHON=jupyter\n",
    "%env PYSPARK_DRIVER_PYTHON_OPTS=notebook\n",
    "%env PYSPARK_PYTHON=python\n",
    "%env OBJC_DISABLE_INITIALIZE_FORK_SAFETY = YES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7619ed82-a761-40d3-8d58-9d2658c5657b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[tfidf: vector, label: int]\n",
      "+----------------------+----------------------------+---------------+---------------+------------------+--------------------------+---------------+\n",
      "|first(_corrupt_record)|first(featureSubsetStrategy)|first(impurity)|first(maxDepth)|first(minInfoGain)|first(minInstancesPerNode)|first(numTrees)|\n",
      "+----------------------+----------------------------+---------------+---------------+------------------+--------------------------+---------------+\n",
      "|   {'bootstrap': True}|                        auto|           gini|             10|              10.0|                        10|            100|\n",
      "+----------------------+----------------------------+---------------+---------------+------------------+--------------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark_job.py\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, IDF, IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import to_timestamp, from_unixtime, when, col, regexp_replace, lower, udf, concat, lit, first\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import IntegerType, DoubleType, ArrayType, StringType\n",
    "\n",
    "\n",
    "class Stemmer(Transformer, HasInputCol, HasOutputCol):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(Stemmer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def setInputCol(self, value):\n",
    "        \"\"\"\n",
    "        Sets the value of :py:attr:`inputCol`.\n",
    "        \"\"\"\n",
    "        return self._set(inputCol=value)\n",
    "\n",
    "    def setOutputCol(self, value):\n",
    "        \"\"\"\n",
    "        Sets the value of :py:attr:`outputCol`.\n",
    "        \"\"\"\n",
    "        return self._set(outputCol=value)\n",
    "        \n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        \n",
    "        def stem_words(words):\n",
    "            return [PorterStemmer().stem(w) for w in words]\n",
    "        \n",
    "        t = ArrayType(StringType())\n",
    "        out_col = self.getOutputCol()\n",
    "        in_col = df[self.getInputCol()]\n",
    "        return df.withColumn(out_col, udf(f=stem_words, returnType=t)(in_col))\n",
    "    \n",
    "    \n",
    "def create_spark_session(is_local: bool):\n",
    "    \"\"\"\n",
    "    Create spark session.\n",
    "    Input: \n",
    "        is_local (bool) - True if run in local machine, False if in AWS EMR\n",
    "    Output: \n",
    "        spark (SparkSession) - spark session connected to AWS EMR cluster\n",
    "    \"\"\"\n",
    "    \n",
    "    if is_local:\n",
    "        spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"ML\") \\\n",
    "        .config('spark.driver.host','127.0.0.1') \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.memory\", \"1g\") \\\n",
    "        .config('spark.executor.cores', '2') \\\n",
    "        .getOrCreate()\n",
    "        \n",
    "    else:\n",
    "        spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"ML\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    return spark\n",
    "\n",
    "\n",
    "def preprocessing(spark, input_path: str, is_test: bool):\n",
    "    \"\"\"\n",
    "    Preprocess data\n",
    "    Input:\n",
    "        spark - SparkSession\n",
    "        input_path (string) - path to data\n",
    "        is_test (bool) - if True, limit data to 100 \n",
    "    Output:\n",
    "        df (DataFrame) - DataFrame from SparkSession\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # read file\n",
    "    df = spark.read.json(path=input_path)\n",
    "    \n",
    "    #change columns\n",
    "    df = df.withColumn(\"label\", df[\"overall\"].cast(IntegerType()))\n",
    "    # df = df.withColumn(\"asin\", df[\"asin\"].cast(DoubleType()))\n",
    "    # df = df.withColumn(\"vote\", when(col(\"vote\") == \"none\", None).otherwise(col(\"vote\")).cast(\"double\"))\n",
    "    # df = df.withColumn('style', df[\"style.Format:\"]) \n",
    "    # df = df.withColumn('unixReviewTime', from_unixtime(df['unixReviewTime']))\n",
    "    # df = df.withColumn(\"reviewTime\", to_timestamp(df.reviewTime, 'MM d, yyyy'))\n",
    "\n",
    "    df = df.drop('asin', 'image', 'reviewTime', 'reviewerID', 'reviewerName', 'reviewTime', 'unixReviewTime', 'overall'\\\n",
    "                , 'style', 'summary', 'verified', 'vote') \n",
    "    \n",
    "    df = df.withColumn('reviewText', regexp_replace('reviewText', \"\\S+@\\S+\\s\", ' '))\\\n",
    "                .withColumn('reviewText', regexp_replace('reviewText', \"\\n|\\t\", ' '))\\\n",
    "                .withColumn('reviewText', regexp_replace('reviewText', \"\\S*\\d+\\S*\", ' '))\\\n",
    "                .withColumn('reviewText', regexp_replace('reviewText', \"\\s\\W*\\w\\W*\\s\", ' '))\\\n",
    "                .withColumn('reviewText', regexp_replace('reviewText', \"\\W+\", ' '))\n",
    "    \n",
    "    df = df.withColumn('reviewText', lower('reviewText'))\n",
    "    # fill null values\n",
    "    df = df.na.fill(\"null\",[\"reviewText\"]) \n",
    "    \n",
    "    # limit data if test\n",
    "    if is_test:\n",
    "        df = df.limit(100)\n",
    "        \n",
    "    preprocessing_pipeline = Pipeline(\n",
    "        stages=[\n",
    "            RegexTokenizer(inputCol=\"reviewText\", outputCol=\"words\", pattern=\"\\\\W\"),\n",
    "            StopWordsRemover(inputCol='words', outputCol='words_cleaned'),\n",
    "            Stemmer(inputCol='words_cleaned', outputCol='words_stemmed'),\n",
    "            CountVectorizer(inputCol=\"words_stemmed\", outputCol=\"term_freq\", minDF=10.0), #, maxDF=0.5),\n",
    "            IDF(inputCol=\"term_freq\", outputCol=\"tfidf\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preprocessing_model = preprocessing_pipeline.fit(df)\n",
    "    df = preprocessing_model.transform(df)\n",
    "        \n",
    "    return df\n",
    "    \n",
    "\n",
    "def process_book_data(spark, input_path, output_path, with_indexers: bool, is_test: bool, is_local: bool):\n",
    "    \"\"\"\n",
    "    Process data - modeling, CrossValidation, Tuning Parameters\n",
    "    Input:\n",
    "        spark - SparkSession\n",
    "        input_path (string) - path to data\n",
    "        output_path (string) - path for output (best parameters)\n",
    "        with_indexers (bool) - if True use additional Indexer: StringIndexer, VectorIndexer (it has no added value in this case)\n",
    "        is_test (bool) - if True, limit data to 100 \n",
    "        is_local (bool) - True if run in local machine, False if in AWS EMR    \n",
    "    \"\"\"\n",
    "    \n",
    "    df = preprocessing(spark, input_path, is_test)\n",
    "\n",
    "    \n",
    "    df = df.select(['tfidf', 'label'])\n",
    "    \n",
    "    print(df.cache())\n",
    "    train_set, test_set = df.randomSplit([0.75, 0.25], seed=123)\n",
    "\n",
    "    if with_indexers is True:\n",
    "        \"\"\"\n",
    "        In our case this is not needed, it has no added value\n",
    "        \"\"\"\n",
    "#         labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(df)\n",
    "#         featureIndexer = VectorIndexer(inputCol=\"tfidf\", outputCol=\"indexedFeatures\", maxCategories=4).fit(df)\n",
    "\n",
    "#         r_forest = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n",
    "#         evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "#         param_grid = ParamGridBuilder() \\\n",
    "#         .addGrid(r_forest.maxDepth, [10, 20]) \\\n",
    "#         .addGrid(r_forest.maxBins, [5, 10]) \\\n",
    "#         .addGrid(r_forest.numTrees, [50, 100, 200]) \\\n",
    "#         .addGrid(r_forest.impurity, ['gini','entropy']) \\\n",
    "#         .build()\n",
    "                \n",
    "#         cv = CrossValidator(estimator=r_forest,\n",
    "#                             estimatorParamMaps=param_grid,\n",
    "#                             evaluator=evaluator,\n",
    "#                             numFolds=5)\n",
    "\n",
    "#         labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=labelIndexer.labels)\n",
    "\n",
    "#         pipeline = Pipeline(stages=[labelIndexer, \n",
    "#                                     featureIndexer, \n",
    "#                                     cv,\n",
    "#                                     labelConverter\n",
    "#                                    ])\n",
    "        cv_stage = 2\n",
    "\n",
    "    else:\n",
    "        \"\"\"\n",
    "        Choose model\n",
    "        \"\"\"\n",
    "        r_forest = RandomForestClassifier(labelCol=\"label\", featuresCol=\"tfidf\")\n",
    "\n",
    "        \"\"\"\n",
    "        Choose metric\n",
    "        \"\"\"\n",
    "        evaluator = MulticlassClassificationEvaluator(\n",
    "            labelCol=\"label\", \n",
    "            predictionCol=\"prediction\", \n",
    "            metricName=\"accuracy\")\n",
    "\n",
    "        \"\"\"\n",
    "        Choose Hyperparameters for tuning\n",
    "        \"\"\"\n",
    "        param_grid = ParamGridBuilder() \\\n",
    "        .addGrid(r_forest.bootstrap, [True]) \\\n",
    "        .addGrid(r_forest.maxDepth, [5, 15]) \\\n",
    "        .addGrid(r_forest.maxBins, [2, 6]) \\\n",
    "        .addGrid(r_forest.numTrees, [50, 100]) \\\n",
    "        .addGrid(r_forest.impurity, ['gini','entropy']) \\\n",
    "        .addGrid(r_forest.featureSubsetStrategy, ['auto']) \\\n",
    "        .addGrid(r_forest.minInstancesPerNode , [10, 25]) \\\n",
    "        .addGrid(r_forest.minInfoGain , [10, 25]) \\\n",
    "        .build()\n",
    "        \n",
    "        # .addGrid(r_forest.minWeightFractionPerNode, [0]) \\\n",
    "        \n",
    "        \"\"\"\n",
    "        Definde CV and number of folds\n",
    "        \"\"\"\n",
    "        cv = CrossValidator(estimator=r_forest,\n",
    "                            estimatorParamMaps=param_grid,\n",
    "                            evaluator=evaluator,\n",
    "                            numFolds=3)\n",
    "\n",
    "\n",
    "        pipeline = Pipeline(stages=[cv])\n",
    "        cv_stage = 0\n",
    "\n",
    "\n",
    "    model = pipeline.fit(train_set)\n",
    "    predictions = model.transform(test_set)\n",
    "    \n",
    "    \"\"\"\n",
    "    Get best params from model and write it to file\n",
    "    \"\"\"\n",
    "    hyperparams = model.stages[cv_stage].getEstimatorParamMaps()[np.argmax(model.stages[cv_stage].avgMetrics)]\n",
    "    hyper_list = []\n",
    "\n",
    "    for i in range(len(hyperparams.items())):\n",
    "        hyper_name = re.search(\"name='(.+?)'\", str([x for x in hyperparams.items()][i])).group(1)\n",
    "        hyper_value = [x for x in hyperparams.items()][i][1]\n",
    "\n",
    "        hyper_list.append({hyper_name: hyper_value})\n",
    "\n",
    "    # Read into Spark DataFrame\n",
    "    params = spark.read.json(spark.sparkContext.parallelize(hyper_list))\n",
    "    # Eliminate null rows\n",
    "    params = params.agg(*[first(x, ignorenulls=True) for x in params.columns])\n",
    "    \n",
    "    if not is_local:\n",
    "        params.write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\n",
    "            path = output_path, mode = \"overwrite\")\n",
    "        \n",
    "    else:\n",
    "        params.show()\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function\n",
    "    Define variables here\n",
    "    WITH_INDEXERS (bool) - if True use additional Indexer: StringIndexer, VectorIndexer (it has no added value in this case)\n",
    "    IS_TEST (bool) - if True, limit data to 100 \n",
    "    IS_LOCAL (bool) - True if run in local machine, False if in AWS EMR    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    WITH_INDEXERS = False\n",
    "    IS_TEST = True\n",
    "    IS_LOCAL = True\n",
    "    \n",
    "    if IS_LOCAL:\n",
    "        INPUT_PATH = r'C:\\Users\\dominik.brys\\OneDrive - Accenture\\DB\\PW-Big_Data\\Projekt\\Books_5\\Books_5_limited_v2.json'\n",
    "        OUTPUT_PATH = r'C:\\Users\\dominik.brys\\OneDrive - Accenture\\DB\\PW-Big_Data\\Projekt\\output\\model'\n",
    "    else:\n",
    "        bucket = 's3://emr-project-v1-db'\n",
    "        input_file = '/Books_5.json'\n",
    "        output_file = '/output_params'\n",
    "        INPUT_PATH = bucket + input_file\n",
    "        OUTPUT_PATH = bucket + output_file\n",
    "    \n",
    "    spark = create_spark_session(IS_LOCAL)\n",
    "    process_book_data(spark, INPUT_PATH, OUTPUT_PATH, WITH_INDEXERS, IS_TEST, IS_LOCAL)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee5ea7ce-dcce-4280-a20f-c18fafa16d69",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkSession' object has no attribute 'sharedState'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DOMINI~1.BRY\\AppData\\Local\\Temp/ipykernel_19788/2341926107.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# spark.stop()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# df.persist('DISK_ONLY')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msharedState\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcacheManager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcacheQuery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'SparkSession' object has no attribute 'sharedState'"
     ]
    }
   ],
   "source": [
    "#OPTIONAL\n",
    "# params.show()\n",
    "# spark.stop()\n",
    "# df.persist('DISK_ONLY')\n",
    "spark.sharedState.cacheManager.cacheQuery()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f4d03e-53a9-46d6-800f-3acce1ceb7cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
